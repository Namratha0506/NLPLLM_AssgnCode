This Code in repository focuses on financial sentiment analysis using a fine-tuned BERT model on a combined dataset from FiQA and Financial PhraseBank. The dataset includes financial sentences labeled as positive, negative, or neutral, allowing the model to learn sentiment-specific language used in finance-related texts. The code performs end-to-end steps like loading and cleaning the data, balancing the class distribution, preprocessing with lemmatization and stopword removal, and converting sentences into BERT tokenized inputs. It then fine-tunes the bert-base-uncased model using PyTorch and Hugging Face Transformers, with training and validation accuracy tracked across multiple epochs. Performance is evaluated using accuracy, classification report, confusion matrix, and test samples. The final model achieves around 89% test accuracy, with strong F1-scores across all sentiment classes. Sample predictions show the model generalizes well to unseen financial content. The code is modular and well-commented for easy reuse or extension. It can be adapted for other domain-specific sentiment tasks by updating the dataset and fine-tuning parameters.

This project is licensed under the MIT License, allowing users to freely use, modify, and distribute the code with proper attribution. It comes without warranty, and is intended for educational and research purposes.
